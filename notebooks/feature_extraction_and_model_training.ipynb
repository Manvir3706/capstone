{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from os import path, mkdir\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from feature_extractor import FeaturesWriter, get_features_loader\n",
    "from utils.utils import register_logger\n",
    "from utils.load_model import load_feature_extractor\n",
    "from features_loader import FeaturesLoader\n",
    "from network.TorchUtils import TorchModel\n",
    "from network.anomaly_detector_model import AnomalyDetector, custom_objective, RegularizedLoss\n",
    "from utils.callbacks import DefaultModelCallback, TensorBoardCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_every = 50  # log the writing of clips every n steps\n",
    "log_file = None  # set logging file\n",
    "num_workers = 4  # define the number of workers used for loading the videos\n",
    "\n",
    "cudnn.benchmark = True\n",
    "register_logger(log_file=log_file)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions of features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/ubuntu/repos/llm-rag/data/Anomaly-Videos-Part-1/test'  # path to the video dataset\n",
    "clip_length = 16  # define the length of each input sample\n",
    "frame_interval = 1 # define the sampling interval between framesq\n",
    "batch_size = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-26 14:16:42,545 Found 2 video files in /home/ubuntu/repos/llm-rag/data/Anomaly-Videos-Part-1/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "data_loader, data_iter = get_features_loader(dataset_path,\n",
    "                                            clip_length,\n",
    "                                            frame_interval,\n",
    "                                            batch_size,\n",
    "                                            num_workers,\n",
    "                                            \"clip\"\n",
    "                                            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip \n",
    "from lavis.models import load_model_and_preprocess\n",
    "from torchvision.transforms import ToPILImage\n",
    "import chromadb\n",
    "\n",
    "class ClipEncoder:\n",
    "    def __init__(self, dataset_path, clip_length, caption_model_type, frame_interval, batch_size, num_workers):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.dataset_path = dataset_path\n",
    "        self.clip_length = clip_length\n",
    "        self.frame_interval = frame_interval\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "        self.data_loader, self.data_iter = get_features_loader(dataset_path,\n",
    "                                                                    clip_length,\n",
    "                                                                    frame_interval,\n",
    "                                                                    batch_size,\n",
    "                                                                    num_workers,\n",
    "                                                                    \"clip\"\n",
    "                                                                    )\n",
    "        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.caption_model, self.vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\",\n",
    "                                                                               model_type=caption_model_type,\n",
    "                                                                               is_eval=True,\n",
    "                                                                               device=device)\n",
    "    def encode_image(self, idx):\n",
    "        frame_tensor = self.data_loader[idx][0].permute(1, 0, 2, 3)\n",
    "        with torch.no_grad():\n",
    "            frame_embeddings = self.model.encode_image(frame_tensor.cuda())\n",
    "        return frame_embeddings\n",
    "\n",
    "    def get_all_image_embeddings(self):\n",
    "        embeddings = []\n",
    "        for idx in range(len(self.data_loader)):\n",
    "            emb = self.encode_image(idx)\n",
    "            embeddings.append(emb)\n",
    "        return embeddings\n",
    "    \n",
    "    def get_captions(self):\n",
    "        captions_list = []  \n",
    "        for idx in range(len(self.data_loader)):  \n",
    "            frame_tensor = self.data_loader[idx][0].permute(1, 0, 2, 3)\n",
    "            pil_image = ToPILImage()(frame_tensor[0]) \n",
    "            image = self.vis_processors[\"eval\"](pil_image).unsqueeze(0).to(self.device)\n",
    "            generated_captions = self.caption_model.generate({\"image\": image})  \n",
    "            captions_list.append(generated_captions)\n",
    "        return captions_list\n",
    "\n",
    "    def get_all_caption_embeddings(self, captions_list):\n",
    "        # Future improvements: Maybe multiple captions per image; Think about a way how to add anomalous captions / features\n",
    "        caption_embeddings = []\n",
    "        if captions_list:\n",
    "            for caption_set in captions_list:\n",
    "                if caption_set:\n",
    "                    for caption in caption_set:\n",
    "                        if caption and len(caption) > 0:\n",
    "                            with torch.no_grad():\n",
    "                                try:\n",
    "                                    caption_features = clip.tokenize(caption).to(self.device)\n",
    "                                    caption_embedding = self.model.encode_text(caption_features)\n",
    "                                    caption_embeddings.append(caption_embedding)\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error encoding text for caption: {caption}\")\n",
    "                                    print(f\"Error details: {e}\")\n",
    "        return caption_embeddings\n",
    "    \n",
    "    def get_or_create_chroma_collection(self, collection_name):\n",
    "        try:\n",
    "            collection = self.chroma_client.get_or_create_collection(collection_name)\n",
    "            return collection\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating collection: {collection_name}\")\n",
    "            print(f\"Error details: {e}\")\n",
    "\n",
    "    def upload_embeddings_to_chroma(self, collection_name, embeddings, ids, documents=None, metadata=None):\n",
    "        if not len(embeddings) == len(ids):\n",
    "            raise ValueError(\"embeddings and ids must have the same length\")\n",
    "\n",
    "        collection = self.get_or_create_chroma_collection(collection_name)\n",
    "\n",
    "        for emb, id_ in zip(embeddings, ids):\n",
    "            #print(emb.shape)\n",
    "            print(id_)\n",
    "            try:\n",
    "                collection.add(documents=documents, embeddings=emb, metadatas=metadata, ids=id_)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to add document with ID {id_}: {str(e)}\")                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-26 12:34:23,991 Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-03-26 12:34:23,993 Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-03-26 12:34:24,004 Found 1 video files in /home/ubuntu/repos/llm-rag/data/normal-videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-26 12:34:33,729 Missing keys []\n",
      "2024-03-26 12:34:33,730 load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP/blip_coco_caption_base.pth\n"
     ]
    }
   ],
   "source": [
    "normal_videos = \"/home/ubuntu/repos/llm-rag/data/normal-videos\"\n",
    "encoder = ClipEncoder(normal_videos, clip_length, 'base_coco', frame_interval, batch_size, num_workers)\n",
    "#captions_list = encoder.get_captions()\n",
    "image_embeddings = encoder.get_all_image_embeddings()\n",
    "#captions = encoder.get_captions()\n",
    "#caption_embeddings = encoder.get_all_caption_embeddings(captions_list)\n",
    "\n",
    "\n",
    "document_ids = []\n",
    "for i in range(len(encoder.data_loader)):\n",
    "    item = encoder.data_loader.getitem_from_raw_video(idx=i)  \n",
    "    for j in range(clip_length):\n",
    "        document_ids.append(str(item[3] + '_' + str(item[1]) + '-' + str(j)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal_Videos001_x264_0\n",
      "Failed to add document with ID Normal_Videos001_x264_0: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_1\n",
      "Failed to add document with ID Normal_Videos001_x264_1: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_2\n",
      "Failed to add document with ID Normal_Videos001_x264_2: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_3\n",
      "Failed to add document with ID Normal_Videos001_x264_3: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_4\n",
      "Failed to add document with ID Normal_Videos001_x264_4: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_5\n",
      "Failed to add document with ID Normal_Videos001_x264_5: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_6\n",
      "Failed to add document with ID Normal_Videos001_x264_6: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_7\n",
      "Failed to add document with ID Normal_Videos001_x264_7: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_8\n",
      "Failed to add document with ID Normal_Videos001_x264_8: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_9\n",
      "Failed to add document with ID Normal_Videos001_x264_9: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_10\n",
      "Failed to add document with ID Normal_Videos001_x264_10: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_11\n",
      "Failed to add document with ID Normal_Videos001_x264_11: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_12\n",
      "Failed to add document with ID Normal_Videos001_x264_12: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_13\n",
      "Failed to add document with ID Normal_Videos001_x264_13: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_14\n",
      "Failed to add document with ID Normal_Videos001_x264_14: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_15\n",
      "Failed to add document with ID Normal_Videos001_x264_15: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_16\n",
      "Failed to add document with ID Normal_Videos001_x264_16: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_17\n",
      "Failed to add document with ID Normal_Videos001_x264_17: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_18\n",
      "Failed to add document with ID Normal_Videos001_x264_18: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_19\n",
      "Failed to add document with ID Normal_Videos001_x264_19: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_20\n",
      "Failed to add document with ID Normal_Videos001_x264_20: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_21\n",
      "Failed to add document with ID Normal_Videos001_x264_21: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_22\n",
      "Failed to add document with ID Normal_Videos001_x264_22: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_23\n",
      "Failed to add document with ID Normal_Videos001_x264_23: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_24\n",
      "Failed to add document with ID Normal_Videos001_x264_24: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_25\n",
      "Failed to add document with ID Normal_Videos001_x264_25: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_26\n",
      "Failed to add document with ID Normal_Videos001_x264_26: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_27\n",
      "Failed to add document with ID Normal_Videos001_x264_27: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_28\n",
      "Failed to add document with ID Normal_Videos001_x264_28: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_29\n",
      "Failed to add document with ID Normal_Videos001_x264_29: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_30\n",
      "Failed to add document with ID Normal_Videos001_x264_30: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_31\n",
      "Failed to add document with ID Normal_Videos001_x264_31: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_32\n",
      "Failed to add document with ID Normal_Videos001_x264_32: Number of embeddings 16 must match number of ids 1\n",
      "Normal_Videos001_x264_33\n",
      "Failed to add document with ID Normal_Videos001_x264_33: Number of embeddings 16 must match number of ids 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "document_ids = np.array(document_ids)\n",
    "batched_ids = np.array_split(document_ids, len(document_ids) // clip_length)\n",
    "\n",
    "emb_list = [emb.tolist() for emb in image_embeddings]\n",
    "encoder.upload_embeddings_to_chroma(\"normal_videos\", emb_list, batched_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(emb_list[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
