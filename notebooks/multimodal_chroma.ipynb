{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from os import path, mkdir\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from feature_extractor import FeaturesWriter, get_features_loader\n",
    "from utils.utils import register_logger\n",
    "from utils.load_model import load_feature_extractor\n",
    "from features_loader import FeaturesLoader\n",
    "from network.TorchUtils import TorchModel\n",
    "from network.anomaly_detector_model import AnomalyDetector, custom_objective, RegularizedLoss\n",
    "from utils.callbacks import DefaultModelCallback, TensorBoardCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_every = 50  # log the writing of clips every n steps\n",
    "log_file = None  # set logging file\n",
    "num_workers = 4  # define the number of workers used for loading the videos\n",
    "\n",
    "cudnn.benchmark = True\n",
    "register_logger(log_file=log_file)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/ubuntu/repos/llm-rag/data/Anomaly-Videos-Part-1/test'  # path to the video dataset\n",
    "clip_length = 16  # define the length of each input sample\n",
    "frame_interval = 1 # define the sampling interval between framesq\n",
    "batch_size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import numpy as np\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from torchvision.transforms import ToPILImage\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "from chromadb.utils.data_loaders import ImageLoader\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "class ClipEncoder:\n",
    "    def __init__(self, dataset_path, clip_length, caption_model_type, frame_interval, batch_size, num_workers):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.dataset_path = dataset_path\n",
    "        self.clip_length = clip_length\n",
    "        self.frame_interval = frame_interval\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "        self.data_loader, self.data_iter = get_features_loader(dataset_path,\n",
    "                                                                    clip_length,\n",
    "                                                                    frame_interval,\n",
    "                                                                    batch_size,\n",
    "                                                                    num_workers,\n",
    "                                                                    \"clip\"\n",
    "                                                                    )\n",
    "        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.caption_model, self.vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\",\n",
    "                                                                               model_type=caption_model_type,\n",
    "                                                                               is_eval=True,\n",
    "                                                                               device=device)\n",
    "    def encode_image(self, idx):\n",
    "        frame_tensor = self.data_loader[idx][0].permute(1, 0, 2, 3)\n",
    "        with torch.no_grad():\n",
    "            frame_embeddings = self.model.encode_image(frame_tensor.cuda())\n",
    "        return frame_embeddings\n",
    "\n",
    "    def get_all_image_embeddings(self):\n",
    "        embeddings = []\n",
    "        for idx in range(len(self.data_loader)):\n",
    "            emb = self.encode_image(idx)\n",
    "            embeddings.append(emb)\n",
    "        return embeddings\n",
    "    \n",
    "    def export_tensor_to_np(self):\n",
    "        arr = []\n",
    "        for idx in range(len(self.data_loader)):\n",
    "            frame_tensor = self.data_loader[idx][0].permute(1, 0, 2, 3)\n",
    "            pil_image = ToPILImage()(frame_tensor[0]) \n",
    "            arr.append(np.array(pil_image))\n",
    "        return arr\n",
    "    \n",
    "    def export_tensor_to_base64(self):\n",
    "        arr = []\n",
    "        for idx in range(len(self.data_loader)):\n",
    "            frame_tensor = self.data_loader[idx][0].permute(1, 0, 2, 3)\n",
    "            pil_image = ToPILImage()(frame_tensor[0])\n",
    "            buffered = BytesIO()\n",
    "            pil_image.save(buffered, format=\"JPEG\")\n",
    "            img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "            arr.append(img_str)\n",
    "        return arr\n",
    "    \n",
    "    def get_captions(self):\n",
    "        captions_list = []  \n",
    "        for idx in range(len(self.data_loader)):  \n",
    "            frame_tensor = self.data_loader[idx][0].permute(1, 0, 2, 3)\n",
    "            pil_image = ToPILImage()(frame_tensor[0]) \n",
    "            image = self.vis_processors[\"eval\"](pil_image).unsqueeze(0).to(self.device)\n",
    "            generated_captions = self.caption_model.generate({\"image\": image})  \n",
    "            captions_list.append(generated_captions)\n",
    "        return captions_list\n",
    "\n",
    "    def get_all_caption_embeddings(self, captions_list):\n",
    "        # Future improvements: Maybe multiple captions per image; Think about a way how to add anomalous captions / features\n",
    "        caption_embeddings = []\n",
    "        if captions_list:\n",
    "            for caption_set in captions_list:\n",
    "                if caption_set:\n",
    "                    for caption in caption_set:\n",
    "                        if caption and len(caption) > 0:\n",
    "                            with torch.no_grad():\n",
    "                                try:\n",
    "                                    caption_features = clip.tokenize(caption).to(self.device)\n",
    "                                    caption_embedding = self.model.encode_text(caption_features)\n",
    "                                    caption_embeddings.append(caption_embedding)\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error encoding text for caption: {caption}\")\n",
    "                                    print(f\"Error details: {e}\")\n",
    "        return caption_embeddings\n",
    "    \n",
    "    def generate_document_ids(self):\n",
    "        document_ids = []\n",
    "        for i in range(len(self.data_loader)):\n",
    "            item = self.data_loader.getitem_from_raw_video(idx=i)  \n",
    "            for j in range(self.clip_length):\n",
    "                document_ids.append(str(item[3] + '_' + str(item[1]) + '-' + str(j)))\n",
    "\n",
    "        batched_ids = [document_ids[i:i+clip_length] for i in range(0, len(document_ids), clip_length)]\n",
    "        \n",
    "        return document_ids, batched_ids\n",
    "    \n",
    "    def get_or_create_chroma_collection(self, collection_name, embedding_function=None, data_loader=None):\n",
    "        if embedding_function:\n",
    "            try:\n",
    "                collection = self.chroma_client.get_or_create_collection(name=collection_name, embedding_function=embedding_function, data_loader=data_loader)\n",
    "                return collection\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating collection: {collection_name}\")\n",
    "                print(f\"Error details: {e}\")\n",
    "        else:    \n",
    "            try:\n",
    "                collection = self.chroma_client.get_or_create_collection(collection_name)\n",
    "                return collection\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating collection: {collection_name}\")\n",
    "                print(f\"Error details: {e}\")\n",
    "\n",
    "\n",
    "    def upload_embeddings_to_chroma(self, collection_name, img_data, ids, multi_modal= False, captions=None, documents=None, metadata=None):\n",
    "        if multi_modal:\n",
    "            if not len(img_data) == len(ids):\n",
    "                raise ValueError(\"data and ids must have the same length\")\n",
    "            \n",
    "            embedding_function = OpenCLIPEmbeddingFunction(\"ViT-H-14\",\"laion2b_s32b_b79k\" )\n",
    "            data_loader = ImageLoader()\n",
    "            \n",
    "            collection = self.get_or_create_chroma_collection(collection_name, embedding_function, data_loader)\n",
    "            print(\"Multi Modal Collection created\")\n",
    "\n",
    "            for frame, id_ in zip(img_data, ids):\n",
    "                try:\n",
    "                    collection.add(images=frame[0], metadatas=metadata, ids=id_[0])\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to add document with ID {id_}: {str(e)}\")         \n",
    "        \n",
    "        else:\n",
    "            if not len(img_data) == len(ids):\n",
    "                raise ValueError(\"data and ids must have the same length\")\n",
    "\n",
    "            collection = self.get_or_create_chroma_collection(collection_name)\n",
    "\n",
    "            for emb, id_ in zip(img_data, ids):\n",
    "                try:\n",
    "                    collection.add(documents=documents, embeddings=emb, metadatas=metadata, ids=id_)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to add document with ID {id_}: {str(e)}\")                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-28 16:46:59,543 Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-03-28 16:46:59,544 Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-03-28 16:46:59,552 Found 41 video files in /home/ubuntu/repos/llm-rag/data/normal-videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:42<00:00, 14.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-28 16:47:50,018 Missing keys []\n",
      "2024-03-28 16:47:50,018 load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP/blip_coco_caption_base.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/anomaly_310/lib/python3.10/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    }
   ],
   "source": [
    "normal_videos = \"/home/ubuntu/repos/llm-rag/data/normal-videos\"\n",
    "encoder = ClipEncoder(normal_videos, clip_length, 'base_coco', frame_interval, batch_size, num_workers)\n",
    "np_arr = encoder.export_tensor_to_np()\n",
    "\n",
    "document_ids = []\n",
    "for i in range(len(encoder.data_loader)):\n",
    "    item = encoder.data_loader.getitem_from_raw_video(idx=i)  \n",
    "    for j in range(clip_length):\n",
    "        document_ids.append(str(item[3] + '_' + str(item[1]) + '-' + str(j)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ClipEncoder(dataset_path, clip_length, 'base_coco', frame_interval, batch_size, num_workers)\n",
    "base64_arr = encoder.export_tensor_to_base64()\n",
    "\n",
    "document_ids = []\n",
    "for i in range(len(encoder.data_loader)):\n",
    "    item = encoder.data_loader.getitem_from_raw_video(idx=i)  \n",
    "    for j in range(clip_length):\n",
    "        document_ids.append(str(item[3] + '_' + str(item[1]) + '-' + str(j)))  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [document_ids[i:i+clip_length] for i in range(0, len(document_ids), clip_length)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-28 17:17:58,041 Loaded ViT-H-14 model config.\n",
      "2024-03-28 17:18:06,277 Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).\n",
      "Multi Modal Collection created\n"
     ]
    }
   ],
   "source": [
    "encoder.upload_embeddings_to_chroma(\"multi-modal-norm\", np_arr, batched_ids, multi_modal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-28 18:49:35,547 Loaded ViT-H-14 model config.\n",
      "2024-03-28 18:49:43,466 Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).\n",
      "{'ids': [['Normal_Videos058_x264_53-0', 'Normal_Videos058_x264_60-0', 'Normal_Videos058_x264_59-0']], 'distances': [[1.59778634273598, 1.59778634273598, 1.59778634273598]], 'embeddings': None, 'metadatas': None, 'documents': None, 'uris': [[None, None, None]], 'data': [[None, None, None]]}\n"
     ]
    }
   ],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "collection = encoder.chroma_client.get_collection(\"multi-modal-norm\", embedding_function=OpenCLIPEmbeddingFunction(\"ViT-H-14\",\"laion2b_s32b_b79k\"), data_loader=ImageLoader())\n",
    "\n",
    "retrieved = collection.query(query_texts=\"Lockers at a highschool\", include=['data', 'distances'], n_results=3)\n",
    "print(retrieved)\n",
    "# # TODO: Add a way to visualize the retrieved images\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly_310",
   "language": "python",
   "name": "anomaly_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
